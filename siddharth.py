# -*- coding: utf-8 -*-
"""Siddharth.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13s_G4EeGnAbe544hghtIb39NBUgWeGyc
"""

from google.colab import files


uploaded = files.upload()

from google.colab import files


uploaded = files.upload()

# Commented out IPython magic to ensure Python compatibility.
#import libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, accuracy_score

import pickle

# %matplotlib inline

#import data
df_travel=pd.read_csv("TRAVEL.csv")
df_travel.info()

df_travel.head()

results = df_travel["Claim"].value_counts(dropna=False)
results

print("Proportion of claims that are Yes: " , results[1]/(results[0] + results[1]))

hist=df_travel.hist()

boxplot1 = df_travel.boxplot(column=['Age'])

boxplot2 = df_travel.boxplot(column=['Commision (in value)'])

boxplot3 = df_travel.boxplot(column=['Duration'])

boxplot4 = df_travel.boxplot(column=['Net Sales'])

df_travel.describe()

df_travel["Agency"].value_counts(dropna=False)

df_travel["Agency Type"].value_counts(dropna=False)

df_travel["Distribution Channel"].value_counts(dropna=False)

df_travel["Product Name"].value_counts(dropna=False)

df_travel["Destination"].value_counts(dropna=False)

df_travel["Gender"].value_counts(dropna=False)

sns.pairplot(df_travel, hue="Claim")

#Correlation Plot
fig, ax = plt.subplots(figsize=(15, 10))
sns.heatmap(df_travel.corr(), annot=True, ax=ax)

df_travel1 = df_travel[ df_travel["Age"]<=100]
df_travel2 = df_travel1[ df_travel1["Commision (in value)"]<=250]
df_travel3 = df_travel2[ df_travel2["Duration"]<=1000]

# setup the encoder
le = LabelEncoder()
df_travel4=df_travel3.copy()

# do the encoding for Agency
le.fit(df_travel4['Agency'])
df_travel4['Agency encoded'] = le.transform(df_travel4['Agency'])
df_travel5 = pd.concat([df_travel4, pd.get_dummies(df_travel4['Agency'])], axis=1)

# do the encoding for Agency Type
le.fit(df_travel5['Agency Type'])
df_travel5['Agency Type encoded'] = le.transform(df_travel5['Agency Type'])
df_travel6 = pd.concat([df_travel5, pd.get_dummies(df_travel5['Agency Type'])], axis=1)

# do the encoding for Distribution Channel
le.fit(df_travel6['Distribution Channel'])
df_travel6['Distribution Channel encoded'] = le.transform(df_travel6['Distribution Channel'])
df_travel7 = pd.concat([df_travel6, pd.get_dummies(df_travel6['Distribution Channel'])], axis=1)

# do the encoding for Product Name
le.fit(df_travel7['Product Name'])
df_travel7['Product Name encoded'] = le.transform(df_travel7['Product Name'])
df_travel8 = pd.concat([df_travel7, pd.get_dummies(df_travel7['Product Name'])], axis=1)

# do the encoding for Destination
le.fit(df_travel8['Destination'])
df_travel8['Destination encoded'] = le.transform(df_travel8['Destination'])
df_travel9 = pd.concat([df_travel8, pd.get_dummies(df_travel8['Destination'])], axis=1)

# do the encoding for Gender
#first, address NaN
df_travel9['Gender'].fillna("Other", inplace=True)
le.fit(df_travel9['Gender'])
df_travel9['Gender encoded'] = le.transform(df_travel9['Gender'])
df_travel10 = pd.concat([df_travel9, pd.get_dummies(df_travel9['Gender'])], axis=1)

# encode the Claim variable. 1 for "Yes", 0 for "No"
df_travel10["Claim_Target"] = np.where(df_travel10["Claim"]=='Yes', 1, 0)
df_travel10.head()

#check imbalance dataset
df_travel10["Claim_Target"].value_counts(dropna=False)

df_col=pd.DataFrame(df_travel10.columns)
df_col.columns=["Column_Name"]

# output to csv because we can't get it to print all of it
df_col.to_csv("travel_ins_col_names.csv")

#split dataset between features and target
features=['Duration', 'Net Sales', 'Commision (in value)', 'Age', 'AEGON', 'ART', 'MAX LIFE INSURANCE', 'KOTAK', 'BHARTI AXA', 'RELIANCE', 'ICICI', 'LIC', 'NEW INDIA', 'UNITED INDIA', 'HDFC ERGO', 'ORIENTAL', 'HDFC', 'TATA AIG', 'SBI', 'Airlines', 'Travel Agency', 'Offline', 'Online', '1 way Comprehensive Plan', '2 way Comprehensive Plan', '24 Protect', 'Annual Gold Plan', 'Annual Silver Plan', 'Annual Travel Protect Gold', 'Annual Travel Protect Platinum', 'Annual Travel Protect Silver', 'Basic Plan', 'Bronze Plan', 'Cancellation Plan', 'Child Comprehensive Plan', 'Comprehensive Plan', 'Gold Plan', 'Individual Comprehensive Plan', 'Premier Plan', 'Rental Vehicle Excess Insurance', 'Silver Plan', 'Single Trip Travel Protect Gold', 'Single Trip Travel Protect Platinum', 'Single Trip Travel Protect Silver', 'Spouse or Parents Comprehensive Plan', 'Ticket Protector', 'Travel Cruise Protect', 'Travel Cruise Protect Family', 'Value Plan', 'ALBANIA', 'ANGOLA', 'ARGENTINA', 'ARMENIA', 'AUSTRALIA', 'AUSTRIA', 'AZERBAIJAN', 'BAHRAIN', 'BANGLADESH', 'BARBADOS', 'BELARUS', 'BELGIUM', 'BENIN', 'BERMUDA', 'BHUTAN', 'BOLIVIA', 'BOSNIA AND HERZEGOVINA', 'BOTSWANA', 'BRAZIL', 'BRUNEI DARUSSALAM', 'BULGARIA', 'CAMBODIA', 'CAMEROON', 'CANADA', 'CAYMAN ISLANDS', 'CHILE', 'CHINA', 'COLOMBIA', 'COSTA RICA', 'CROATIA', 'CYPRUS', 'CZECH REPUBLIC', 'DENMARK', 'DOMINICAN REPUBLIC', 'ECUADOR', 'EGYPT', 'ESTONIA', 'ETHIOPIA', 'FAROE ISLANDS', 'FIJI', 'FINLAND', 'FRANCE', 'FRENCH POLYNESIA', 'GEORGIA', 'GERMANY', 'GHANA', 'GREECE', 'GUADELOUPE', 'GUAM', 'GUATEMALA', 'GUINEA', 'GUINEA-BISSAU', 'GUYANA', 'HONG KONG', 'HUNGARY', 'ICELAND', 'INDIA', 'INDONESIA', 'IRAN, ISLAMIC REPUBLIC OF', 'IRELAND', 'ISRAEL', 'ITALY', 'JAMAICA', 'JAPAN', 'JORDAN', 'KAZAKHSTAN', 'KENYA', 'KOREA, REPUBLIC OF', 'KUWAIT', 'KYRGYZSTAN', "LAO PEOPLE'S DEMOCRATIC REPUBLIC", 'LATVIA', 'LEBANON', 'LIBYAN AORIENTAL JAMAHIRIYA', 'LITHUANIA', 'LUXEMBOURG', 'MACAO', 'MACEDONIA, THE FORMER YUGOSLAV REPUBLIC OF', 'MALAYSIA', 'MALDIVES', 'MALI', 'MALTA', 'MAURITIUS', 'MEXICO', 'MOLDOVA, REPUBLIC OF', 'MONGOLIA', 'MOROCCO', 'MYANMAR', 'NAMIBIA', 'NEPAL', 'NETHERLANDS', 'NEW CALEDONIA', 'NEW ZEALAND', 'NIGERIA', 'NORTHERN MARIANA ISLANDS', 'NORWAY', 'OMAN', 'PAKISTAN', 'PANAMA', 'PAPUA NEW GUINEA', 'PERU', 'PHILIPPINES', 'POLAND', 'PORTUGAL', 'PUERTO RICO', 'QATAR', 'REPUBLIC OF MONTENEGRO', 'REUNION', 'ROMANIA', 'RUHDFCAN FEDERATION', 'RWANDA', 'SAMOA', 'SAUDI AORIENTALIA', 'SENEGAL', 'SERBIA', 'SEYCHELLES', 'SIERRA LEONE', 'SINGAPORE', 'SLOVENIA', 'SOLOMON ISLANDS', 'SOUTH AFRICA', 'SPAIN', 'SRI LANKA', 'SWEDEN', 'SWITZERLAND', 'TAIWAN, PROVINCE OF CHINA', 'TAJIKISTAN', 'TANZANIA, UNITED REPUBLIC OF', 'THAILAND', 'TRINIDAD AND TOBAGO', 'TUNISIA', 'TURKEY', 'TURKMENISTAN', 'TURKS AND CAICOS ISLANDS', 'UGANDA', 'UKRAINE', 'UNITED AORIENTAL EMIRATES', 'UNITED KINGDOM', 'UNITED STATES', 'URUGUAY', 'UZBEKISTAN', 'VANUATU', 'VENEZUELA', 'VIET NAM', 'VIRGIN ISLANDS, U.S.', 'ZAMBIA', 'ZIMBABWE', 'F', 'M', 'Other']
target = "Claim_Target"

X = df_travel10.loc[:, features]
y = df_travel10[target]
X.head()

#split between training and test set, default 4:1
X_train1, X_test1, y_train, y_test = train_test_split(X, y)
X_train1.shape, X_test1.shape, y_train.shape, y_test.shape

# Do scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train1)
X_test=scaler.transform(X_test1)

# Do PCA before and after SMOTE to see effects
pca_2d_before = PCA(n_components=2)
pca_2d_before.fit(X_train)
Z_train_pca_2d_before = pca_2d_before.transform(X_train)

print(Z_train_pca_2d_before.shape)
print(y_train.shape)
print(type(Z_train_pca_2d_before), type(y_train))

mask0 = (y_train == 0)
mask1 = (y_train == 1)

fig, ax = plt.subplots()

# ax.scatter(Z[:, 0], Z[:, 1]) # Z[:, 0] - first col, Z[:, 1], 2nd col

# numpy slicing arr[row, col]
ax.scatter(Z_train_pca_2d_before[mask0, 0], Z_train_pca_2d_before[mask0, 1], label='No=0')
ax.scatter(Z_train_pca_2d_before[mask1, 0], Z_train_pca_2d_before[mask1, 1], label='Yes=1')

ax.set_xlabel('PC 0')
ax.set_ylabel('PC 1')
ax.legend()
plt.show()

# Import necessary libraries
import numpy as np
from imblearn.over_sampling import SMOTE

# Assuming you have X_train and y_train defined earlier in your code
# X_train should be your feature matrix, and y_train should be your target labels

# Initialize SMOTE
sm = SMOTE(random_state=2)

# Resample the data
X_train_resampled, y_train_resampled = sm.fit_resample(X_train, y_train)

# Check the shape and types of the resampled data
print(X_train_resampled.shape, y_train_resampled.shape)
print(type(X_train_resampled), type(y_train_resampled))

y_train.sum()

# pca
pca_2d = PCA(n_components=2)
pca_2d.fit(X_train)
Z_train_pca_2d = pca_2d.transform(X_train)

mask0 = (y_train == 0)
mask1 = (y_train == 1)

fig, ax = plt.subplots()

# numpy slicing arr[row, col]
ax.scatter(Z_train_pca_2d[mask0, 0], Z_train_pca_2d[mask0, 1], label='No=0')
ax.scatter(Z_train_pca_2d[mask1, 0], Z_train_pca_2d[mask1, 1], label='Yes=1')

ax.set_xlabel('PC 0')
ax.set_ylabel('PC 1')
ax.legend()
plt.show()

num_input_features = len(features)
num_input_features

#code to clear old keras model
import keras
keras.backend.clear_session()

from keras.models import Sequential
from keras.layers import Dense
# from keras.layers import Dropout

model = Sequential()

#start with 400 nodes
model.add(Dense(400, activation='relu', input_shape=(num_input_features,)))

#input num_input_features, 199.
#output 199 as well.
model.add(Dense(200, activation='relu'))
# model.add(Dropout(0.2))

#input 199. output 50
model.add(Dense(50, activation='relu'))
# model.add(Dropout(0.2))

# softmax converts a set of outputs to probabilities that add up to 1
num_classes=1 #didn't work with 1, so try two
model.add(Dense(num_classes, activation='sigmoid'))

model.summary()

from keras.callbacks import TensorBoard
from keras.callbacks import EarlyStopping
from keras.callbacks import ModelCheckpoint
from keras.optimizers import RMSprop
from keras.utils import to_categorical
import time

batch_size = 10
epochs = 20

#y_train_binary = to_categorical(y_train)
#y_test_binary = to_categorical(y_test)

tensorboard = TensorBoard(log_dir='./logs/insurance_mlp/%d' % time.time())
earlystop = EarlyStopping(monitor='val_loss', patience=1, mode='auto')
checkpoint = ModelCheckpoint('travel_insurance_model.h5')

model.compile(loss='binary_crossentropy',
              optimizer=RMSprop(),
              metrics=['accuracy']) # Tensorboard will display
                                    # acc in addition to loss

history = model.fit(X_train, y_train,
                    batch_size=batch_size,
                    epochs=epochs,
                    verbose=1,
                    callbacks=[tensorboard, earlystop, checkpoint],
                    validation_data=(X_test, y_test))

y_pred_prob = model.predict(X_test)
y_pred = y_pred_prob.argmax(axis=1)

# Computing metrics
from sklearn.metrics import confusion_matrix, classification_report
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d')
plt.show()

print(classification_report(y_test, y_pred,zero_division=1))

accuracy = accuracy_score(y_test, y_pred)
print(accuracy)

import pickle

pickle.dump(model, open('Sid.pkl','wb'))